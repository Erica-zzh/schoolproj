---
title: "STA457 A1"
author: "Zhiheng(Erica) Zhou"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

# Ch1
## 1.6 (p39)

### (a)

\begin{align*}
x_t = \beta_1 + \beta_{2}t +w_t\\
\mu_{x_t} = E[x_t] = \beta_1+\beta_{2}t
\end{align*}
Since the mean function depends on time t, the time series is not stationary.

### (b)

\begin{align*}
y_t = x_t - x_{t-1} 
=(\beta_1 + \beta_2 t +w_t) - (\beta_1 + \beta_2(t-1)+ w_{t-1})
=\beta_2 + w_t -w_{t-1}
\end{align*}

\begin{align*}
\mu_{y_t} = E[y_t] = E[\beta_2 + w_t + w_{t-1}] = \beta_2
\end{align*}

\begin{align*}
\gamma(t+h, t) &= cov(y_{t+h}, y_t)\\ 
&= cov(w_{t+h},w_t) +cov(-w_{t+h-1}, w_t) + cov(w_{t+h}, w_{t-1}) + cov(-w_{t+h-1}, w_{t-1})\\
&= 
\begin{cases}
2\sigma_{w}^2 & \quad \text{h=0}\\ 
-\sigma_{w}^2 & \quad \text{|h| = 1}\\ 
0 & \quad \text{|h|> 1}
\end{cases}
\end{align*}

Because the mean function does not depend on t; and the autocovariance function only depends on h the difference in time, y_t is stationary.

### (c)

\begin{align*}
v_t = \frac{1}{2q+1} \sum_{j=-q}^{q} x_{t-j}\\
\end{align*}

\begin{align*}
\frac{1}{2q+1} \sum_{j=-q}^{q} x_{t-j} &= \frac{1}{2q+1}(x_{t+q} + x_{t+q-1} +...+x_{t+q-2q})\\
&= \frac{1}{2q+1} (\beta_1 +\beta_2(t+q) + w_{t+q} + \beta_1 + \beta_2(t+q-1) + w_{t+q-1}+...+\beta_1+\beta_2(t+q-2q) + w_{t+q-2q})\\
&= \frac{1}{2q+1}[(2q+1)\beta_1+(2q+1)t\beta_2 +(2q+1)q\beta_2 - q(2q+1)\beta_2 + \sum_{i=0}^{2q} w_{t+q-i} ] \\
&= \beta_1 +\beta_2 t + \frac{1}{2q+1}\sum_{i=0}^{2q} w_{t+q-i}
\end{align*}


\begin{align*}
\mu_{v_t} = E[v_t] &= E[\frac{1}{2q+1} \sum_{j=-q}^{q} x_{t-j}]\\
&= E[\beta_1 +\beta_2 t]\\
&= \beta_1 + \beta_2 t 
\end{align*}


\begin{align*}
\gamma(t+h, t) &= cov(v_{t+h}, v_t)\\
&= cov ( \frac{1}{2q+1}\sum_{i=0}^{2q} w_{t+h+q-i},
 \frac{1}{2q+1}\sum_{i=0}^{2q} w_{t+q-i})\\
&= 
\begin{cases}
\frac{2q+1-h}{(2q+1)^2} \sigma_w^2 & \quad \text{0 $\leq$ |h|$\leq$ q}\\
0 & \quad \text{|h|> q}
\end{cases}
\end{align*}

## 1.19 (p42)

### (a) 
\begin{align*}
\mu_{x_t} = E[x_t] &= E[\mu + w_t +\theta w_{t-1}] \\
&= \mu + E[w_t] + E[\theta w_{t-1}]\\
&= \mu
\end{align*}

### (b)

\begin{align*}
\gamma(t+h, t) &= cov(\mu+w_{t+h}+\theta w_{t+h-1}, \mu + w_t +\theta w_{t-1})\\
&= cov(w_{t+h}, w_t) + cov(w_{t+h}, \theta w_{t-1}) + cov(\theta w_{t+h-1}, w_t)+ cov(\theta w_{t+h-1}, \theta w_{t-1})\\
&= 
\begin{cases}
(1+\theta^2)\sigma_w^2 & \quad \text{h=0}\\
\theta\sigma_w^2 & \quad \text{|h|=1}\\
0 & \quad \text{|h|> 1}
\end{cases}
\end{align*}

Thus, the statement holds.

### (c)

We showed $\mu_{x_t} = \mu$, which does not depend on t, and $\gamma(t+h, t)$ depends only on h the difference in time. Thus, $x_t$ is stationary for all $\theta$. 

### (d)

\begin{align*}
var(\bar{x}) &= \frac{1}{n^2} cov(\sum{t=1}^{n} x_t, \sum{s=1}^{n} x_s)\\
&= \frac{1}{n^2} (n\gamma(0) + (n-1)\gamma(1) + ...+\gamma(n-1) + (n-1)\gamma(-1) + ...+\gamma(1-n))\\
&= \frac{1}{n^2} (n(1+\theta^2) + (n-1)\theta\sigma_w^2 + (n-1)\theta\sigma_w^2)\\
&= \frac{1}{n^2}(n(1+\theta^2)\sigma_w^2 + 2(n-1)\theta\sigma_w^2)\\
&= 
\begin{cases}
\frac{2(1+\frac{n-1}{n})}{n}\sigma_w^2 & \quad \text{$\theta$=1 (i)}\\
\frac{1}{n}\sigma_w^2 & \quad \text{$\theta$=0 (ii)}\\
\frac{2(1-\frac{n-1}{n})}{n}\sigma_w^2 & \quad \text{$\theta$=-1 (iii)}
\end{cases}
\end{align*}


### (e)

When n becomes extremely large, the variances in (i) and (ii) will approach to 0, and the variance in (iii) will be approximately 0. Therefore, the estimate in (iii) will be more accurate than the accuracy in (i) and (ii).

\newpage

## 1.20 (p42)

### (a)

```{r}
set.seed(457)
w_a = rnorm(500,0,1) # the white noise with sample size 500
acf(w_a, 20, plot=T, ylab="ACF") # compute the ACF
```
From the ACF plot, we see that the estimated ACF is statistically close to 0 across all the lags other than lag 0. The theoretical ACF for white noise series is given by $\rho(0)$ = 1 when h = 0 and $\rho(h)$ = 0 when h $\neq$ 0. We can say that the ACF got from simulation are nearly the same.

### (b)

```{r, cache=TRUE}
set.seed(457)
w_b = rnorm(50,0,1) # the white noise with sample size 50
acf(w_b, 20, plot=T) # compute the ACF
```

As the sample size becomes smaller, the autocorrelations are respectively greater because time series analysis usually needs a large sample size to converge to the actual value of the parameter, and a smaller sample size will cause a larger variation. Thus, the ACF with sample size 50 are less consistent with the theoretical ACF the one with sample size 500.

\newpage

# Ch2

## 2.1 (p70)

### (a)
```{r}
install.packages("astsa")
library(astsa)
trend <- time(jj) - 1970
q <- factor(cycle(jj)) 
mod1 <- lm(log(jj) ~ 0 + trend + q, na.action=NULL) 
summary(mod1)
```

The model is $x_t = log(y_t) = 0.167172 + 1.052793Q_1(t)+1.080916Q_2(t)+1.151024Q_3(t)+0.882266Q_4(t) +w_t$


### (b)

If the model is correct, the estimated average annual increase in the logged
earnings per share is $1.052793+1.080916 +1.151024+0.882266 = 4.166999$.

### (c)

If the model is correct, the average logged earnings rate decrease for $(1.151024-0.882266)/1.151024 = 23.34947$ percent from the third quarter to the fourth quarter.


### (d)

```{r, cache=TRUE}
mod2 <- lm(log(jj)~ trend + q, na.action=NULL) # assume intercept
summary(mod2)
```

If we include an intercept in the model, we fail to estimate the coefficient for Q1. Also, the estimates of Q2 and Q3 will be less significant. Especially, the estimate of Q2 is not statistically significant.

### (e)

```{r}
plot(log(jj), main = "Fig.1 Observed values & Fitted values") # the observed
lines(fitted(mod1), col="red") # the fitted
legend("bottomright",c("fitted", "observed"),lty = 1,col = 2:0)
res <- resid(mod1) # residuals
plot(res, main = "Fig.2 Residuals")
```

On both Fig.1 and Fig.2 we see that the model fits the data well. Fig.1 shows that the observed values and fitted values are always close to each other across time. Fig.2 shows that the residuals distributed like white noise with mean around 0.  

\newpage

## 2.3 (p70)

### (a)

```{r}
set.seed(457)
par(mfrow=c(2,2), mar=c(2.5,2.5,0,0)+.5, mgp=c(1.6,.6,0)) # set up
for (i in 1:4){
x = ts(cumsum(rnorm(100,.01,1))) # data
regx = lm(x~0+time(x), na.action=NULL) # regression
plot(x, ylab='Random Walk w Drift') # plots
abline(a=0, b=.01, col=2, lty=2) # true mean (red - dashed)
abline(regx, col=4) # fitted line (blue - solid)
}
```

### (b)

```{r}
set.seed(457)
par(mfrow=c(2,2), mar=c(2.5,2.5,0,0)+.5, mgp=c(1.6,.6,0)) # set up
for (i in 1:4){
w <- rnorm(100,0,1)
t <- 1:100
y <- ts(.01*t + w) # data
regy = lm(y~0+t, na.action=NULL) # regression
plot(y, ylab='Linear Trend plus Noise') # plots
abline(a=0, b=.01, col=2, lty=2) # true mean (red - dashed)
abline(regy, col=4) # fitted line (blue - solid)
}
```
### (c)

We can see that the data in (b) fits the linear model fits the data in (b) much better than the data in (a). It is because a random walk contains accumulated white noises across time, while the white noises are independent in a trend stationary process.

## 2.10 

### (a)

```{r}
plot(gas, main="Weekly Time Series of Oil and Gas", ylim= c(0,350), ylab="Dollars per Barrel(Oil)/Cents per Gallon(Gas)")
lines(oil, col="red", main="Weekly Time Series of Gas", ylab="Cents per Gallon")
legend("topright",c("oil", "gas"),lty = 1,col = 2:0)
```
They are similar to the random walk data. All of them has an upward trend. Thus, I think neither of the data is not stationary because of the existence of trend. A time series with trend is not stationary because the trend affects the mean, variance and other properties of the time series.

### (b)

\begin{align*}
Footnote 2: log(1+p) = p-\frac{p^2}{2}+\frac{p^3}{3}-... \quad \text{-1<p$\leq$1}\\
\text{If p is near 0, the higher-order terms in the expansion are negligible. }
\end{align*}

\begin{align*}
y_t &= \frac{x_t-x_{t-1}}{x_{t-1}}\\
x_t &= (1+y_t)x_{t-1}\\
1+y_t &= \frac{x_t}{x_{t-1}}\\
log(1+y_t) & = log(\frac{x_t}{x_{t-1}})\\
y_t -\frac{y_t^2}{2}+\frac{y_t^3}{3}-... &= log(x_t)-log(x_{t-1})\\
y_t &= log(x_t) - log(x_{t-1})\\
&= \nabla log(x_t) \quad \text{,where $x_t$ is oil or gas price series}
\end{align*}


### (c)

```{r}
plot(diff(log(gas)), main="Diff(log(gas)) & Diff(log(oil))", ylab = "diff(log(gas/oil))")
lines(diff(log(oil)), col="red", ylab= "diff(gas)")
legend("topright",c("oil", "gas"),lty = 1,col = 2:0)
```
```{r}
acf(diff(log(oil)),100, main="ACF Diff(oil)")
acf(diff(log(gas)),100, main="ACF Diff(gas)")
```
The data looks like white noise. The trend effect has been eliminated. Therefore, the time series are stationary except for gas price around 2006 and oil price around 2009. The ACFs are mostly close to 0 and are similar to the ACF of white noise. Thus, the oil and gas price series are mostly stationary.

### (d)

```{r}
ccf(diff(log(oil)),diff(log(gas)), 100, ylab="CCF")
```
The CCF shows that there exists cross-correlation between oil prices and gas prices. The cross-correlation reaches the peak at lag 0. Thus, we can expect large positive cross-correlation in small lags. That is, the increase in gas price leads to an increase in oil price within small lags. 

\newpage

# Ch3

## 3.20 (p158)


```{r}
set.seed(457)
rep(NA, 3) -> phi -> theta -> sigma2

for (i in 1:3){
  x_t <- arima.sim(list(ar=.9,ma= -.9),n=500) 
  fit <- arima(x_t, order = c(1,0,1))
  phi[i] = fit$coef[1]
  theta[i] = fit$coef[2]
  sigma2[i] = fit$sigma2
}

plot(x_t, main = "simulated data")
acf2(x_t)


cbind("phi" = phi, "theta" = theta, "sigma2" = sigma2)
```

The simulated data distribute like white noise. It is reasonable because $x_t$ = $w_t$ is a solution to the equation. We did not see significant ACF or PACF at lag 1, thus, ARMA(1,1) is not the real model. The $\phi$s and $\theta$s are nearly negative to each other but the actual values can vary. For example, there is a big difference in the values of the estimates between the first and third simulation. 

## 3.32

```{r}
plot(oil)
acf2(oil, 50)
```

As we have noticed before, the oil data has an increasing trend. We can also see this through the ACF. The series has large ACF at small lags and decreasing overtime, which represents trend. 

```{r}
# take difference to remove trend
plot(diff(oil), ylab = 'Differenced oil price')
```
Then we difference the data once to remove the trend. The data mostly looks like white noise. Therefore, we set the degree of difference at 1, i.e. d = 1. 

```{r}
grow_oil <- diff(log(oil))
plot(grow_oil, ylab = "Differenced Log Price")
```

Although we have detrended the data, it still shows heteroskedasticity. Thus, we take logarithm to reduce heteroskedasticity. We see from the plot that the variance seems more constant than before. 

```{r, cache=TRUE}
acf2(grow_oil, 50)
```

We notice that the ACF and PACF both have 4 significant spikes. Thus, we can set the ar = 4 and ma = 4.

```{r}
# take log to reduce hetroskedasticity
sarima(log(oil), 4,1,4)
```

We see that the residuals, ACF and Q-Q plots look generally good, but some of the p-values are below or at the significant level. It may be because of the outliers in the data. 

## 3.35 (p161)

### (a)

```{r}
plot(sales, main ="Sales of 150 months")
acf2(sales)
```

The sales data has an increasing trend, and we can also see that through the ACF plot. Therefore, it needed to be transferred. 

```{r}
plot(diff(sales))
acf2(diff(sales))
```

We can use a growth rate transformation, and the data looks good. The first and second spikes of PACF are significant. Thus, we can set the ar parameter at 2. The first four spikes of ACF are significant, and we can set the ma parameter at 4. Also, recall that we have differenced the data once, and thus, the difference parameter is 1. Then we fit a ARIMA(2, 1, 4) model.

```{r}
sarima(sales, 2, 1, 4, no.constant=TRUE)
```

After fitting the ARIMA(2,1,4) model, we see from the diagnosis that the residuals are distributed as white noise with constant variance. There is no significant autocorrelation between residuals. Normality is also satisfied. The p-values are above the significance level.


### (b)

```{r}
ccf(diff(sales), diff(lead), ylab = "CCF")
```

On the CCF plot, we see that there is a significant positive cross-correlation at lag 3. It indicates that the sales growth series at t + 3 is associated with the lead growth series at t. The two series move in the same direction because of the positiveness.

```{r}
lag2.plot(diff(lead), diff(sales),5)
```

On the lag plots, we see that the lowess fits are linear. And the maximum correlation appears at t - 3 (for leads), which again confirm our results before. Therefore, a regression of $\nabla S_t$ on $\nabla L_{t-3}$ is reasonable.

### (c)

```{r}
# first include only lead 
df <- ts.intersect(y = diff(sales) , z= stats::lag( diff(lead), -3 ), dframe = TRUE)
fit.y <- lm(y ~ z , data = df, na.action=NULL)

# Based on the residual, choose x_t because x_t explains some of the residuals.
plot(resid(fit.y))
acf2(resid(fit.y),50)

# Based on the ACF and PACF, assume x_t is ARMA(1,4) process
sarima(df$y,1,0,4, xreg = df$z)
```

To determine the process of x, we can first include only lead in the regression. Based on the residual, choose $x_t$ because $x_t$ explains some of the residuals. Based on the ACF and PACF, assume $x_t$ is ARMA(1,4) process
We see that the residual, ACF, Q-Q plot and the p-values are all good. Therefore, it is reasonable to assume $x_t$ is an ARMA(1,4) process.

## 3.39 (p162)

```{r}
phi = c(rep(0,11),.8)
ACF = ARMAacf(ar=phi, ma=.5, 50)[-1] # [-1] removes 0 lag
PACF = ARMAacf(ar=phi, ma=.5, 50, pacf=TRUE)
par(mfrow=c(1,2))
plot(ACF, type="h", xlab="LAG", ylim=c(-.4,.8)); abline(h=0)
```

